{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f559ce36",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The requirement is to use Neural Network models to predict the traffic demand given the historical data. In this case I am using a single **Logistic Regression Model** with **1 hidden layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142b097",
   "metadata": {},
   "source": [
    "First, we need to import all the libraries that we are going to need to complete the task. As a library, we will be using **pyTorch**. Also, for the data preprocessing we are using **numpy**, and for the visualization we will be using **pyplot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526f4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137680e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f685e05",
   "metadata": {},
   "source": [
    "### Importing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b6efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('train.npz')\n",
    "input_train = train['x'] #feature matrix\n",
    "label_train = train['y'] #label matrix\n",
    "location_train = train['locations'] #location matrix### Importing Valuation Data\n",
    "times_train = train['times'] #time matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb1eff",
   "metadata": {},
   "source": [
    "### Importing Valuation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7a0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.load('val.npz')\n",
    "input_val = val['x'] #feature matrix\n",
    "label_val = val['y'] #label matrix\n",
    "location_val = val['locations'] #location matrix\n",
    "times_val = val['times'] #time matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90637e",
   "metadata": {},
   "source": [
    "### Importing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3a6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('test.npz')\n",
    "input_test = test['x'] #feature matrix\n",
    "location_test = test['locations'] #location matrix\n",
    "times_test = test['times'] #time matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af8fd48",
   "metadata": {},
   "source": [
    "## Changing from NumPy Arrays into Tensors\n",
    "\n",
    "Here we are creating tensors from numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a85b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = torch.from_numpy(input_train)\n",
    "labels_train = torch.from_numpy(label_train)\n",
    "\n",
    "inputs_val = torch.from_numpy(input_val)\n",
    "labels_val = torch.from_numpy(label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31867ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(inputs_train.float(), labels_train.float())\n",
    "val_ds = TensorDataset(inputs_val.float(), labels_val.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b601c38",
   "metadata": {},
   "source": [
    "### Creating DataLoader\n",
    "\n",
    "Now, we are creating dataloaders to load the data in batches (in our case we will be using batches of size 100).\n",
    "\n",
    "Since the training data is often sorted by the target labels, or at least it is not random, therefore, it is crucial for us to choose random data items for our batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137b5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "input_size = 8*49\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf00a45",
   "metadata": {},
   "source": [
    "### Shape\n",
    "\n",
    "Here, we can see that our items in the train_loader have the shape of (100(number of items in the batch), 8, 49). But for ou rfurther operations such as matrix multiplications, this shape is going to be invalid for us. Therefore, we need to reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89575bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items.shape: torch.Size([100, 8, 49])\n",
      "inputs.shape: torch.Size([100, 392])\n"
     ]
    }
   ],
   "source": [
    "for items, labels in train_loader:\n",
    "    print('items.shape:', items.shape)\n",
    "    inputs = items.reshape(-1, 8*49)\n",
    "    print('inputs.shape:', inputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace89c6e",
   "metadata": {},
   "source": [
    "The size of the hidden layer is going to be 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9043772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = inputs.shape[-1]\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ef379",
   "metadata": {},
   "source": [
    "### Layer Creation\n",
    "\n",
    "Next, we will create a nn.Linear object that is going to be our hidden layer. The size is already defined to be 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fcb6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = nn.Linear(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e465ac18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_outputs.shape: torch.Size([100, 64])\n"
     ]
    }
   ],
   "source": [
    "L1_outputs = L1(inputs)\n",
    "print('layer1_outputs.shape:', L1_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a555255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1_outputs_direct = inputs @ L1.weight.t() + L1.bias\n",
    "L1_outputs_direct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32c231",
   "metadata": {},
   "source": [
    "The image of vectors of size 392 are now transformed into intermediate output vectors of lenght 64 after matrix multiplications and addition of bias.\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "L1_outputs and inputs have a linear relationship, where each element of L1_putputs is a weighted sum of elements of unputs. Therefore, layer 1 can only capture linear relationships. That is why we need some kind of function that would make the next relationship between L1 and L2 non-linear.\n",
    "\n",
    "This kind of function is called an activation function and there are typically 5 major functions which are Step, Tanh, ReLU, and leaky ReLU.\n",
    "\n",
    "In our case, we will be using ReLU (Rectified Linear Unit) as an activation function. What it does is it ignores the non-negative numbers, but the negative numbers are transformed to 0. This function is no derivativable, and therefore is a good choice for a function to get rid from linearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "161bc160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min(L1_outputs): -56.520835876464844\n",
      "min(relu_outputs): 0.0\n"
     ]
    }
   ],
   "source": [
    "relu_outputs = F.relu(L1_outputs)\n",
    "print('min(L1_outputs):', torch.min(L1_outputs).item())\n",
    "print('min(relu_outputs):', torch.min(relu_outputs).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad702f",
   "metadata": {},
   "source": [
    "### Creation of Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1af9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2 = nn.Linear(hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53a4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "L2_outputs = L2(relu_outputs)\n",
    "print(L2_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c1824",
   "metadata": {},
   "source": [
    "Now, layer 2 outputs contains a batch of vectors of size 1. Now, we can compute the loss using F.mse_loss (Mean Squared Loss) function and adjust the weights of L1 and L2 using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd69839f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(470.2571, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(L2_outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c646737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded version of layer2(F.relu(layer1(inputs)))\n",
    "outputs = (F.relu(inputs @ L1.weight.t() + L1.bias)) @ L2.weight.t() + L2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dbea19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as layer2(layer1(inputs))\n",
    "outputs2 = (inputs @ L1.weight.t() + L1.bias) @ L2.weight.t() + L2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b29e86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single layer to replace the two linear layers\n",
    "combined_layer = nn.Linear(input_size, 1)\n",
    "\n",
    "combined_layer.weight.data = L2.weight @ L1.weight\n",
    "combined_layer.bias.data = L1.bias @ L2.weight.t() + L2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed5666b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as combined_layer(inputs)\n",
    "outputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d84e6f",
   "metadata": {},
   "source": [
    "### Defining Model\n",
    "\n",
    "Here we will define our own class of Model, that will be extended from nn.Linear model with some minor additions such as reshaping and applying a hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d1f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 8*49)\n",
    "        out = self.linear1(xb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        items, labels = batch \n",
    "        out = self(items)\n",
    "        loss = F.mse_loss(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        items, labels = batch \n",
    "        out = self(items)\n",
    "        loss = F.mse_loss(out, labels)\n",
    "        return {'val_loss': loss}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "318eba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_size, hidden_size=32, out_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "893362a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 392])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for t in model.parameters():\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bf80a",
   "metadata": {},
   "source": [
    "Here from model parameters we can see how the sizes of our batches are changing with each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d65f8",
   "metadata": {},
   "source": [
    "### Evaluate and Fit Functions\n",
    "\n",
    "Now, it is time to define the evaluate and fit functions.\n",
    "\n",
    "The evaluate() function will be checking the performance of the model on the validation loader.\n",
    "\n",
    "The fit() function is going to be a function that is handling the training of the model and applying the backward() process that will be making the model better overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75654c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    \"\"\"Train the model using gradient descent\"\"\"\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ea42e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 341.6239013671875}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0039d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 24.5951\n",
      "Epoch [1], val_loss: 22.1745\n",
      "Epoch [2], val_loss: 21.4285\n",
      "Epoch [3], val_loss: 21.3776\n",
      "Epoch [4], val_loss: 21.8017\n",
      "Epoch [5], val_loss: 20.5626\n",
      "Epoch [6], val_loss: 22.3841\n",
      "Epoch [7], val_loss: 20.2465\n",
      "Epoch [8], val_loss: 22.6555\n",
      "Epoch [9], val_loss: 19.7944\n",
      "Epoch [10], val_loss: 19.8502\n",
      "Epoch [11], val_loss: 20.8647\n",
      "Epoch [12], val_loss: 21.4028\n",
      "Epoch [13], val_loss: 20.2443\n",
      "Epoch [14], val_loss: 19.7907\n",
      "Epoch [15], val_loss: 19.4051\n",
      "Epoch [16], val_loss: 20.3620\n",
      "Epoch [17], val_loss: 19.3020\n",
      "Epoch [18], val_loss: 20.1297\n",
      "Epoch [19], val_loss: 19.6405\n"
     ]
    }
   ],
   "source": [
    "history += fit(20, 1e-4, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f1ef069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 18.6308\n",
      "Epoch [1], val_loss: 18.6759\n",
      "Epoch [2], val_loss: 18.5793\n",
      "Epoch [3], val_loss: 18.5551\n",
      "Epoch [4], val_loss: 18.5984\n",
      "Epoch [5], val_loss: 18.5959\n",
      "Epoch [6], val_loss: 18.6486\n",
      "Epoch [7], val_loss: 18.5891\n",
      "Epoch [8], val_loss: 18.5588\n",
      "Epoch [9], val_loss: 18.5969\n",
      "Epoch [10], val_loss: 18.5723\n",
      "Epoch [11], val_loss: 18.6045\n",
      "Epoch [12], val_loss: 18.5993\n",
      "Epoch [13], val_loss: 18.6262\n",
      "Epoch [14], val_loss: 18.6135\n",
      "Epoch [15], val_loss: 18.5833\n",
      "Epoch [16], val_loss: 18.6069\n",
      "Epoch [17], val_loss: 18.5931\n",
      "Epoch [18], val_loss: 18.6151\n",
      "Epoch [19], val_loss: 18.5812\n"
     ]
    }
   ],
   "source": [
    "history += fit(20, 1e-5, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f126355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 18.5565\n",
      "Epoch [1], val_loss: 18.5551\n",
      "Epoch [2], val_loss: 18.5509\n",
      "Epoch [3], val_loss: 18.5530\n",
      "Epoch [4], val_loss: 18.5494\n",
      "Epoch [5], val_loss: 18.5490\n",
      "Epoch [6], val_loss: 18.5530\n",
      "Epoch [7], val_loss: 18.5514\n",
      "Epoch [8], val_loss: 18.5552\n",
      "Epoch [9], val_loss: 18.5509\n",
      "Epoch [10], val_loss: 18.5516\n",
      "Epoch [11], val_loss: 18.5524\n",
      "Epoch [12], val_loss: 18.5457\n",
      "Epoch [13], val_loss: 18.5507\n",
      "Epoch [14], val_loss: 18.5540\n",
      "Epoch [15], val_loss: 18.5522\n",
      "Epoch [16], val_loss: 18.5506\n",
      "Epoch [17], val_loss: 18.5513\n",
      "Epoch [18], val_loss: 18.5501\n",
      "Epoch [19], val_loss: 18.5498\n"
     ]
    }
   ],
   "source": [
    "history += fit(20, 1e-6, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "743c821b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 18.5492\n",
      "Epoch [1], val_loss: 18.5490\n",
      "Epoch [2], val_loss: 18.5489\n",
      "Epoch [3], val_loss: 18.5489\n",
      "Epoch [4], val_loss: 18.5487\n",
      "Epoch [5], val_loss: 18.5486\n",
      "Epoch [6], val_loss: 18.5486\n",
      "Epoch [7], val_loss: 18.5484\n",
      "Epoch [8], val_loss: 18.5484\n",
      "Epoch [9], val_loss: 18.5483\n",
      "Epoch [10], val_loss: 18.5483\n",
      "Epoch [11], val_loss: 18.5483\n",
      "Epoch [12], val_loss: 18.5483\n",
      "Epoch [13], val_loss: 18.5482\n",
      "Epoch [14], val_loss: 18.5482\n",
      "Epoch [15], val_loss: 18.5481\n",
      "Epoch [16], val_loss: 18.5481\n",
      "Epoch [17], val_loss: 18.5480\n",
      "Epoch [18], val_loss: 18.5480\n",
      "Epoch [19], val_loss: 18.5480\n"
     ]
    }
   ],
   "source": [
    "history += fit(20, 1e-7, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3f91f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [x['val_loss'] for x in history]\n",
    "plt.plot(losses, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727415c",
   "metadata": {},
   "source": [
    "After running a several number of epochs we can see that our val_loss (mse_loss) is on the level of 18.4. Also, from the graph we can see that the model flattened already after about 10 epochs. This is a quite good result for the model.\n",
    "\n",
    "Now let's predict the values on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a97acba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3432],\n",
       "        [1.5512],\n",
       "        [1.3817],\n",
       "        ...,\n",
       "        [4.3431],\n",
       "        [5.5608],\n",
       "        [4.5661]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test = torch.from_numpy(input_test)\n",
    "\n",
    "test = model(inputs_test.float())\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f4f4d",
   "metadata": {},
   "source": [
    "Now, we will save the model for the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2bdb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'chicago.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "609239ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[-0.0157, -0.0224, -0.0335,  ...,  0.0243,  0.0615,  0.0363],\n",
       "                      [ 0.0058,  0.0220, -0.0516,  ..., -0.0410, -0.0263,  0.0249],\n",
       "                      [ 0.0160,  0.0183,  0.0377,  ...,  0.0254,  0.0734,  0.0629],\n",
       "                      ...,\n",
       "                      [-0.0449,  0.0221,  0.0253,  ...,  0.0126,  0.0391,  0.0426],\n",
       "                      [-0.0360,  0.0017, -0.0185,  ...,  0.0313, -0.0124, -0.0118],\n",
       "                      [-0.0208,  0.0081, -0.0315,  ...,  0.0074,  0.0234,  0.0143]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([ 0.0214,  0.0077,  0.0914,  0.0330, -0.0495,  0.0214,  0.0298,  0.0355,\n",
       "                      -0.0634,  0.0434, -0.0226,  0.0031,  0.0023,  0.0908, -0.0465,  0.0310,\n",
       "                       0.0293,  0.0211, -0.0026, -0.0298, -0.0428, -0.0056, -0.0307, -0.0551,\n",
       "                      -0.0117,  0.0438, -0.0282,  0.0184, -0.0402,  0.0194, -0.0464,  0.0121])),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.2972, -0.2054,  0.2014, -0.2817, -0.1684,  0.5170,  0.2868, -0.1495,\n",
       "                       -0.1211,  0.2223, -0.0817, -0.3231, -0.1771,  0.1873, -0.2757,  0.1828,\n",
       "                       -0.2252,  0.1352,  0.1422,  0.2011,  0.2053, -0.1216, -0.3731, -0.3149,\n",
       "                       -0.1572,  0.4822, -0.1552,  0.0998,  0.3310,  0.1740, -0.0784,  0.1326]])),\n",
       "             ('linear2.bias', tensor([0.1043]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23a099",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5976963e",
   "metadata": {},
   "source": [
    "The model was trained on the training and validation sets and showed an mse_loss of around 18.4 (Since, it is squared we can assume that the mean average error would be around sqrt18.4 or 4.3). But, there are still losses in the model. For example, more hidden layers can be added to the model or we can use different activation function. Since, the model had only one hidden layer it is not as efficient as Deep Neural Networks such as CNN and RNN, but for the task, it should give the good result in predicting the traffic demand for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf1e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
